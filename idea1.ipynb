{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Idea 1\n",
    "- We induce subgraph of proteins (or maybe subgraph of proteins + 1 degree of separation away)\n",
    "- Find communities using MCL or Louvain to get us communities of size ~30\n",
    "- Determine if these communities are functional\n",
    "- Find bottlenecks/hub proteins of each complex\n",
    "- This gives us relevant proteins for treatment or disrupting pathways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in graph \n",
    "G = nx.read_weighted_edgelist(\"yeast.txt\",comments=\"#\",nodetype=str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing edges not meeting threshold score. **Need to decide what we are doing with essential nodes.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting edges that don't meet threshold score\n",
    "threshold_score = 700\n",
    "for edge in G.edges: \n",
    "    weight = list(G.get_edge_data(edge[0],edge[1]).values())\n",
    "    if(weight[0] <= threshold_score):\n",
    "        G.remove_edge(edge[0],edge[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relabelling nodes to get rid of 4932 tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = nx.relabel_nodes(G, lambda x: x[5:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting list of all yeast homologs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"human_to_yeast.csv\")\n",
    "homologs = list(set(list(df[\"homolog_systematic_name\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting subgraph induced by yeast homologs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H0 = H.subgraph(homologs).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(H0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Louvain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "louvain = nx.algorithms.community.louvain_communities(H0, resolution=7, seed=123)\n",
    "louvain.sort(key=len, reverse=True)\n",
    "\n",
    "number_of_communities = len(louvain)\n",
    "size_of_communities = [len(community) for community in louvain]\n",
    "\n",
    "print(\"Number of communities: {}\".format(number_of_communities))\n",
    "print(\"Sizes of communities: \", size_of_communities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drawing communities (taking the t biggest communities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from netgraph import Graph\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter that controls how many communities we are interested in\n",
    "t = 3\n",
    "\n",
    "# Generating list of nodes of first t communities\n",
    "nodes = []\n",
    "for i in range(t):\n",
    "  nodes.extend(louvain[i]) \n",
    "\n",
    "\n",
    "# Subgraph induced by these communities\n",
    "H1 = H0.subgraph(nodes).copy()\n",
    "\n",
    "# Dictionary where key is node and value is community index\n",
    "node_to_community = {}\n",
    "for i in range(t):\n",
    "  for node in louvain[i]:\n",
    "    node_to_community[node] = i\n",
    "\n",
    "# Assigning t random colours each of our communities\n",
    "community_to_color = dict([(i, \"#\"+''.join([random.choice('0123456789ABCDEF') for j in range(6)])) for i in range(t)])\n",
    "node_color = {node: community_to_color[community_id] for node, community_id in node_to_community.items()}\n",
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "Graph(H1,\n",
    "      node_color=node_color, node_edge_width=0, edge_alpha=0.1, node_size = 0.5, edge_width = 0.5,\n",
    "      node_layout='community', node_layout_kwargs=dict(node_to_community=node_to_community),\n",
    "      edge_layout='bundled', edge_layout_kwargs=dict(k=2000)\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Centrality\n",
    "\n",
    "The sections below find the most central protein in each community, where each entry in the list is {community_index: (protein, centrality_value)}."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Degree Centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hub = {}\n",
    "for index, community in enumerate(louvain):\n",
    "  sub = H0.subgraph(community).copy()\n",
    "  d = sorted(list(nx.algorithms.centrality.degree_centrality(sub).items()), key=lambda x: x[1], reverse=True)\n",
    "  hub[index] = d[0]\n",
    "\n",
    "print(hub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eigenvector Centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hub = {}\n",
    "for index, community in enumerate(louvain):\n",
    "  sub = H0.subgraph(community).copy()\n",
    "  d = sorted(list(nx.algorithms.centrality.eigenvector_centrality(sub).items()), key=lambda x: x[1], reverse=True)\n",
    "  hub[index] = d[0]\n",
    "\n",
    "print(hub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Betweenness Centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hub = {}\n",
    "for index, community in enumerate(louvain):\n",
    "  sub = H0.subgraph(community).copy()\n",
    "  d = sorted(list(nx.algorithms.centrality.betweenness_centrality(sub).items()), key=lambda x: x[1], reverse=True)\n",
    "  hub[index] = d[0]\n",
    "\n",
    "print(hub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VoteRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nx.algorithms.centrality.voterank(H0, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating graph of where each community is a node and the weight of edges between two communities A and B is equal to the number of communities between A and B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Going through all communities and setting the node attribute to be the index of the community it is in\n",
    "community_dict = {}\n",
    "for index, community in enumerate(louvain):\n",
    "  for node in community:\n",
    "    community_dict[node] = index\n",
    "\n",
    "nx.set_node_attributes(H0, community_dict, \"community\")\n",
    "\n",
    "\n",
    "# Creating basic graph with all our communities as nodes, but no edges yet\n",
    "F = nx.Graph()\n",
    "F.add_nodes_from(range(0, number_of_communities))\n",
    "\n",
    "for (u, v) in H0.edges:\n",
    "  community_i = H0.nodes[u][\"community\"]\n",
    "  community_j = H0.nodes[v][\"community\"]\n",
    "  \n",
    "  # if in different communities\n",
    "  if community_i != community_j:\n",
    "    \n",
    "    # if community graph doesnt already have edge, we have to add the edge\n",
    "    if not F.has_edge(community_i, community_j):\n",
    "      F.add_edge(community_i, community_j, weight = 1)\n",
    "    else:\n",
    "      F[community_i][community_j][\"weight\"] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding max weight (min weight is trivially 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_weight = 1\n",
    "for edge in F.edges:\n",
    "  if F.edges[edge][\"weight\"] > max_weight:\n",
    "    max_weight = F.edges[edge][\"weight\"]\n",
    "\n",
    "print(max_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Betweenness Centrality\n",
    "\n",
    "Output below tells us that community 26 has the highest betweenness centrality (so it could be involved in many interactions etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = list(nx.algorithms.centrality.betweenness_centrality(F).items())\n",
    "print(sorted(d, reverse=True, key=lambda x: x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relating node sizes in the plot to how central the communities are (i.e. how large their betweenness centrality is).\n",
    "# Note that communities with zero centrality disappear from the plot (aka communities which are not connected)\n",
    "node_size = []\n",
    "for i in range(len(d)):\n",
    "  if d[i][1] != 0:\n",
    "    node_size.append(d[i][1] * 200000)\n",
    "nodelist = [node for (node, c) in d if c != 0]\n",
    "\n",
    "# Relating edge opacity to weight.\n",
    "edgelist = [(u, v) for (u, v) in F.edges if u in nodelist and v in nodelist]\n",
    "edge_color = []\n",
    "for edge in edgelist:\n",
    "  # We normalise weight value so that it becomes between 0-1 for alpha values in RGBA\n",
    "  edge_color.append((0, 0, 0, (F.edges[edge][\"weight\"] - 1) / (max_weight - 1)))\n",
    "\n",
    "# Adding weight labels (bc why not, this is easily removed)\n",
    "labels = nx.get_edge_attributes(F,'weight')\n",
    "\n",
    "# Scaling node labels with node sizes\n",
    "\n",
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "pos=nx.spring_layout(F)\n",
    "nx.draw_networkx(F, pos, nodelist=nodelist, edgelist=edgelist, node_size=node_size, edge_color=edge_color, width=3)\n",
    "\n",
    "# Adding weight labels (bc why not, this is easily removed)\n",
    "# labels = nx.get_edge_attributes(F,'weight')\n",
    "# nx.draw_networkx_edge_labels(F, pos, edge_labels=labels, font_size=5)\n",
    "\n",
    "# TODO Scaling node labels with node size\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "604b94c5f01275e050c2c00b5ea4e6efa3f121665dac1e4e1c804cbe8201972f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
